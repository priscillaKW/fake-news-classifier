{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engeneering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook contém a etapa de feature engineering dos conjuntos de treino e teste. Essa etapa se baseia no pré-processamento da etapa de Exploratory Analisys, realizada anteriormente.\n",
    "\n",
    "Ao final dessa etapa, serão gerados dois conjuntos de dados de treino e teste processados, que serão utilizados para treinamento e teste dos modelos de classificação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importação de bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize as TK\n",
    "\n",
    "import nltk\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importação dos datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read datasets\n",
    "train = pd.read_csv('dataset/train.csv', encoding='utf-8')\n",
    "test = pd.read_csv('dataset/test.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existem 20800 exemplo, no dataset de treinamento. Cada exemplo contém 5 atributos, sendo: id, title, author, text, label.\n"
     ]
    }
   ],
   "source": [
    "print(\"Existem {} exemplo, no dataset de treinamento. Cada exemplo contém {} atributos, sendo: \"\n",
    "      .format(train.shape[0],train.shape[1]) + \", \".join(train.columns) + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\r\\nAn Iranian woman has been sentenced ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\r\\nAn Iranian woman has been sentenced ...      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualização das 5 primeiras notícias do conjunto de treino\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existem 5200 exemplo, no dataset de treinamento. Cada exemplo contém 4 atributos, sendo: id, title, author, text, label.\n"
     ]
    }
   ],
   "source": [
    "print(\"Existem {} exemplo, no dataset de treinamento. Cada exemplo contém {} atributos, sendo: \"\n",
    "      .format(test.shape[0],test.shape[1]) + \", \".join(train.columns) + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20800</td>\n",
       "      <td>Specter of Trump Loosens Tongues, if Not Purse...</td>\n",
       "      <td>David Streitfeld</td>\n",
       "      <td>PALO ALTO, Calif.  —   After years of scorning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20801</td>\n",
       "      <td>Russian warships ready to strike terrorists ne...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Russian warships ready to strike terrorists ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20802</td>\n",
       "      <td>#NoDAPL: Native American Leaders Vow to Stay A...</td>\n",
       "      <td>Common Dreams</td>\n",
       "      <td>Videos #NoDAPL: Native American Leaders Vow to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20803</td>\n",
       "      <td>Tim Tebow Will Attempt Another Comeback, This ...</td>\n",
       "      <td>Daniel Victor</td>\n",
       "      <td>If at first you don’t succeed, try a different...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20804</td>\n",
       "      <td>Keiser Report: Meme Wars (E995)</td>\n",
       "      <td>Truth Broadcast Network</td>\n",
       "      <td>42 mins ago 1 Views 0 Comments 0 Likes 'For th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title  \\\n",
       "0  20800  Specter of Trump Loosens Tongues, if Not Purse...   \n",
       "1  20801  Russian warships ready to strike terrorists ne...   \n",
       "2  20802  #NoDAPL: Native American Leaders Vow to Stay A...   \n",
       "3  20803  Tim Tebow Will Attempt Another Comeback, This ...   \n",
       "4  20804                    Keiser Report: Meme Wars (E995)   \n",
       "\n",
       "                    author                                               text  \n",
       "0         David Streitfeld  PALO ALTO, Calif.  —   After years of scorning...  \n",
       "1                      NaN  Russian warships ready to strike terrorists ne...  \n",
       "2            Common Dreams  Videos #NoDAPL: Native American Leaders Vow to...  \n",
       "3            Daniel Victor  If at first you don’t succeed, try a different...  \n",
       "4  Truth Broadcast Network  42 mins ago 1 Views 0 Comments 0 Likes 'For th...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualização das 5 primeiras notícias do conjunto de teste\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamento dos dados\n",
    "Nessa etapa serão realizados alguns tratamentos nos dados de treinamento e teste:\n",
    "* Tratamento de missing values\n",
    "* Conversão de todas as palavras para letras minúsculas\n",
    "* Remoção de caracteres númericos e especiais\n",
    "* Remoção de stopwords\n",
    "* Lematização das palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tratamento de missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remover exemplos com string nula\n",
    "train = train[~train['text'].isnull()]\n",
    "train.shape\n",
    "test = test[~test['text'].isnull()]\n",
    "test.shape\n",
    "\n",
    "# preenche títulos nulos com string vazia\n",
    "train['title'].fillna('',inplace=True)\n",
    "test['title'].fillna('',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conversão para minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['processed_text'] = train['text'].str.lower()\n",
    "train['processed_title'] = train['title'].str.lower()\n",
    "\n",
    "test['processed_text'] = test['text'].str.lower()\n",
    "test['processed_title'] = test['title'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remoção de números, caracteres especiais\n",
    "É necessário realizar uma limpeza no texto para igualar palavras iguais que podem ter sido escritas de forma incorreta ou diferente, palavras que não tem sentido sozinhos como números. Para isso faremos:\n",
    "- remoção de caracteres numéricos\n",
    "- remoção de caracteres especiais e pontuação\n",
    "- remoção de acentos, hífens, apóstrofo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retira os caracteres numéricos \n",
    "train['processed_text'] = train['processed_text'].str.replace('[0-9]', '',regex=True)\n",
    "train['processed_title'] = train['processed_title'].str.replace('[0-9]', '',regex=True)\n",
    "\n",
    "test['processed_text'] = test['processed_text'].str.replace('[0-9]', '',regex=True)\n",
    "test['processed_title'] = test['processed_title'].str.replace('[0-9]', '',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retira os caracteres especiais e pontuação\n",
    "train['processed_text'] = train['processed_text'].str.replace('[?_!%&/.,®€™():]', '',regex=True)\n",
    "train['processed_title'] = train['processed_title'].str.replace('[?_!%&/.,®€™():]', '',regex=True)\n",
    "train['processed_text'] = train['processed_text'].str.replace(\"\\\\r\", \"\").str.replace(\"\\\\n\",\"\")\n",
    "train['processed_title'] = train['processed_title'].str.replace(\"\\\\r\", \"\").str.replace(\"\\\\n\",\"\")\n",
    "\n",
    "test['processed_text'] = test['processed_text'].str.replace('[?_!%&/.,®€™():]', '',regex=True)\n",
    "test['processed_title'] = test['processed_title'].str.replace('[?_!%&/.,®€™():]', '',regex=True)\n",
    "test['processed_text'] = test['processed_text'].str.replace(\"\\\\r\", \"\").str.replace(\"\\\\n\",\"\")\n",
    "test['processed_title'] = test['processed_title'].str.replace(\"\\\\r\", \"\").str.replace(\"\\\\n\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove hífen e apóstrofo, adicionando espaço no lugar\n",
    "train['processed_text'] = train['processed_text'].str.replace('[-“”[—‘’\\']', ' ')\n",
    "train['processed_title'] = train['processed_title'].str.replace('[-“”[—‘’\\']', ' ')\n",
    "\n",
    "test['processed_text'] = test['processed_text'].str.replace('[-“”[—‘’\\']', ' ')\n",
    "test['processed_title'] = test['processed_title'].str.replace('[-“”[—‘’\\']', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remoção de acentos\n",
    "train['processed_text'] = train['processed_text'].str.replace('â', \"a\").str.replace('è', 'e').str.replace('é', 'e').str.replace('í', 'i').str.replace('î', 'i').str.replace('ú', 'u').str.replace('ç', 'c')\n",
    "train['processed_title'] = train['processed_title'].str.replace('â', \"a\").str.replace('è', 'e').str.replace('é', 'e').str.replace('í', 'i').str.replace('î', 'i').str.replace('ú', 'u').str.replace('ç', 'c')\n",
    "\n",
    "test['processed_text'] = test['processed_text'].str.replace('â', \"a\").str.replace('è', 'e').str.replace('é', 'e').str.replace('í', 'i').str.replace('î', 'i').str.replace('ú', 'u').str.replace('ç', 'c')\n",
    "test['processed_title'] = test['processed_title'].str.replace('â', \"a\").str.replace('è', 'e').str.replace('é', 'e').str.replace('í', 'i').str.replace('î', 'i').str.replace('ú', 'u').str.replace('ç', 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remoção de stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista de stopwords em inglês\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "train['processed_text'] = train['processed_text'].apply(lambda x: [item for item in x.split(' ') if item not in stop_words])\n",
    "train['processed_title'] = train['processed_title'].apply(lambda x: [item for item in x.split(' ') if item not in stop_words])\n",
    "\n",
    "test['processed_text'] = test['processed_text'].apply(lambda x: [item for item in x.split(' ') if item not in stop_words])\n",
    "test['processed_title'] = test['processed_title'].apply(lambda x: [item for item in x.split(' ') if item not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lematização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "train['processed_text'] = train['processed_text'].apply(lambda x:  [lemmatizer.lemmatize(w) for w in x])\n",
    "train['processed_title'] = train['processed_title'].apply(lambda x:  [lemmatizer.lemmatize(w) for w in x])\n",
    "\n",
    "test['processed_text'] = test['processed_text'].apply(lambda x:  [lemmatizer.lemmatize(w) for w in x])\n",
    "test['processed_title'] = test['processed_title'].apply(lambda x:  [lemmatizer.lemmatize(w) for w in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remoção de palavras vazias (espaços em branco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['processed_text'] = train['processed_text'].apply(lambda x: [a.strip() for a in x if a.strip()])\n",
    "train['processed_title'] = train['processed_title'].apply(lambda x: [a.strip() for a in x if a.strip()])\n",
    "\n",
    "test['processed_text'] = test['processed_text'].apply(lambda x: [a.strip() for a in x if a.strip()])\n",
    "test['processed_title'] = test['processed_title'].apply(lambda x: [a.strip() for a in x if a.strip()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exclusão das notícias repetidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5118, 7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cria variável de texto, com palavras separadas por vírgula\n",
    "train[\"words_text\"] = train[\"processed_text\"].apply(lambda x: \", \".join(x))\n",
    "test[\"words_text\"] = test[\"processed_text\"].apply(lambda x: \", \".join(x))\n",
    "\n",
    "# removendo casos duplicados\n",
    "train.drop_duplicates(subset=['words_text'], keep='first', inplace=True)\n",
    "train.shape\n",
    "test.drop_duplicates(subset=['words_text'], keep='first', inplace=True)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação de novas features\n",
    "\n",
    "Uma feature foi criada na etapa anterior e contém: a lista de palavras de cada notícia concatenadas em apenas uma string, separadas por vírgula. Essa mesma variável será criada para o campo título.\n",
    "Com base na coluna de texto e título processado, será criada, para cada coluna, uma nova feature com a quantidade de palavras de cada texto e cada título"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"words_title\"] = train[\"processed_title\"].apply(lambda x: \", \".join(x))\n",
    "test[\"words_title\"] = test[\"processed_title\"].apply(lambda x: \", \".join(x))\n",
    "\n",
    "train['processed_word_count'] = train['processed_text'].apply(lambda x: len(x))\n",
    "train['processed_title_word_count'] = train['processed_title'].apply(lambda x: len(x))\n",
    "\n",
    "test['processed_word_count'] = test['processed_text'].apply(lambda x: len(x))\n",
    "test['processed_title_word_count'] = test['processed_title'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removendo textos que ficaram vazios após o tratamento e limpeza\n",
    "train = train[train['processed_word_count']!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets após o tratamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O conjunto de treinamento processado possui 20325 exemplos e 11 colunas:\n"
     ]
    }
   ],
   "source": [
    "print(f\"O conjunto de treinamento processado possui {train.shape[0]} exemplos e \\\n",
    "{train.shape[1]} colunas:\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>processed_title</th>\n",
       "      <th>words_text</th>\n",
       "      <th>words_title</th>\n",
       "      <th>processed_word_count</th>\n",
       "      <th>processed_title_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "      <td>[house, dem, aide, even, see, comey, letter, j...</td>\n",
       "      <td>[house, dem, aide, even, see, comey, letter, j...</td>\n",
       "      <td>house, dem, aide, even, see, comey, letter, ja...</td>\n",
       "      <td>house, dem, aide, even, see, comey, letter, ja...</td>\n",
       "      <td>434</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ever, get, feeling, life, circle, roundabout,...</td>\n",
       "      <td>[flynn, hillary, clinton, big, woman, campus, ...</td>\n",
       "      <td>ever, get, feeling, life, circle, roundabout, ...</td>\n",
       "      <td>flynn, hillary, clinton, big, woman, campus, b...</td>\n",
       "      <td>367</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[truth, might, get, fired, october, tension, i...</td>\n",
       "      <td>[truth, might, get, fired]</td>\n",
       "      <td>truth, might, get, fired, october, tension, in...</td>\n",
       "      <td>truth, might, get, fired</td>\n",
       "      <td>704</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "      <td>[video, civilian, killed, single, u, airstrike...</td>\n",
       "      <td>[civilian, killed, single, u, airstrike, ident...</td>\n",
       "      <td>video, civilian, killed, single, u, airstrike,...</td>\n",
       "      <td>civilian, killed, single, u, airstrike, identi...</td>\n",
       "      <td>307</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\r\\nAn Iranian woman has been sentenced ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[print, iranian, woman, sentenced, six, year, ...</td>\n",
       "      <td>[iranian, woman, jailed, fictional, unpublishe...</td>\n",
       "      <td>print, iranian, woman, sentenced, six, year, p...</td>\n",
       "      <td>iranian, woman, jailed, fictional, unpublished...</td>\n",
       "      <td>89</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \\\n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1   \n",
       "1  Ever get the feeling your life circles the rou...      0   \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1   \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1   \n",
       "4  Print \\r\\nAn Iranian woman has been sentenced ...      1   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0  [house, dem, aide, even, see, comey, letter, j...   \n",
       "1  [ever, get, feeling, life, circle, roundabout,...   \n",
       "2  [truth, might, get, fired, october, tension, i...   \n",
       "3  [video, civilian, killed, single, u, airstrike...   \n",
       "4  [print, iranian, woman, sentenced, six, year, ...   \n",
       "\n",
       "                                     processed_title  \\\n",
       "0  [house, dem, aide, even, see, comey, letter, j...   \n",
       "1  [flynn, hillary, clinton, big, woman, campus, ...   \n",
       "2                         [truth, might, get, fired]   \n",
       "3  [civilian, killed, single, u, airstrike, ident...   \n",
       "4  [iranian, woman, jailed, fictional, unpublishe...   \n",
       "\n",
       "                                          words_text  \\\n",
       "0  house, dem, aide, even, see, comey, letter, ja...   \n",
       "1  ever, get, feeling, life, circle, roundabout, ...   \n",
       "2  truth, might, get, fired, october, tension, in...   \n",
       "3  video, civilian, killed, single, u, airstrike,...   \n",
       "4  print, iranian, woman, sentenced, six, year, p...   \n",
       "\n",
       "                                         words_title  processed_word_count  \\\n",
       "0  house, dem, aide, even, see, comey, letter, ja...                   434   \n",
       "1  flynn, hillary, clinton, big, woman, campus, b...                   367   \n",
       "2                           truth, might, get, fired                   704   \n",
       "3  civilian, killed, single, u, airstrike, identi...                   307   \n",
       "4  iranian, woman, jailed, fictional, unpublished...                    89   \n",
       "\n",
       "   processed_title_word_count  \n",
       "0                          10  \n",
       "1                           7  \n",
       "2                           4  \n",
       "3                           6  \n",
       "4                          10  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualização das cinco primeira notícias presentes no conjunto de treino\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O conjunto de teste processado possui 5118 exemplos e 10 colunas:\n"
     ]
    }
   ],
   "source": [
    "print(f\"O conjunto de teste processado possui {test.shape[0]} exemplos e \\\n",
    "{test.shape[1]} colunas:\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>processed_title</th>\n",
       "      <th>words_text</th>\n",
       "      <th>words_title</th>\n",
       "      <th>processed_word_count</th>\n",
       "      <th>processed_title_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20800</td>\n",
       "      <td>Specter of Trump Loosens Tongues, if Not Purse...</td>\n",
       "      <td>David Streitfeld</td>\n",
       "      <td>PALO ALTO, Calif.  —   After years of scorning...</td>\n",
       "      <td>[palo, alto, calif, year, scorning, political,...</td>\n",
       "      <td>[specter, trump, loosens, tongue, purse, strin...</td>\n",
       "      <td>palo, alto, calif, year, scorning, political, ...</td>\n",
       "      <td>specter, trump, loosens, tongue, purse, string...</td>\n",
       "      <td>765</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20801</td>\n",
       "      <td>Russian warships ready to strike terrorists ne...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Russian warships ready to strike terrorists ne...</td>\n",
       "      <td>[russian, warship, ready, strike, terrorist, n...</td>\n",
       "      <td>[russian, warship, ready, strike, terrorist, n...</td>\n",
       "      <td>russian, warship, ready, strike, terrorist, ne...</td>\n",
       "      <td>russian, warship, ready, strike, terrorist, ne...</td>\n",
       "      <td>152</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20802</td>\n",
       "      <td>#NoDAPL: Native American Leaders Vow to Stay A...</td>\n",
       "      <td>Common Dreams</td>\n",
       "      <td>Videos #NoDAPL: Native American Leaders Vow to...</td>\n",
       "      <td>[video, #nodapl, native, american, leader, vow...</td>\n",
       "      <td>[#nodapl, native, american, leader, vow, stay,...</td>\n",
       "      <td>video, #nodapl, native, american, leader, vow,...</td>\n",
       "      <td>#nodapl, native, american, leader, vow, stay, ...</td>\n",
       "      <td>430</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20803</td>\n",
       "      <td>Tim Tebow Will Attempt Another Comeback, This ...</td>\n",
       "      <td>Daniel Victor</td>\n",
       "      <td>If at first you don’t succeed, try a different...</td>\n",
       "      <td>[first, succeed, try, different, sport, tim, t...</td>\n",
       "      <td>[tim, tebow, attempt, another, comeback, time,...</td>\n",
       "      <td>first, succeed, try, different, sport, tim, te...</td>\n",
       "      <td>tim, tebow, attempt, another, comeback, time, ...</td>\n",
       "      <td>355</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20804</td>\n",
       "      <td>Keiser Report: Meme Wars (E995)</td>\n",
       "      <td>Truth Broadcast Network</td>\n",
       "      <td>42 mins ago 1 Views 0 Comments 0 Likes 'For th...</td>\n",
       "      <td>[min, ago, view, comment, like, first, time, h...</td>\n",
       "      <td>[keiser, report, meme, war, e]</td>\n",
       "      <td>min, ago, view, comment, like, first, time, hi...</td>\n",
       "      <td>keiser, report, meme, war, e</td>\n",
       "      <td>51</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title  \\\n",
       "0  20800  Specter of Trump Loosens Tongues, if Not Purse...   \n",
       "1  20801  Russian warships ready to strike terrorists ne...   \n",
       "2  20802  #NoDAPL: Native American Leaders Vow to Stay A...   \n",
       "3  20803  Tim Tebow Will Attempt Another Comeback, This ...   \n",
       "4  20804                    Keiser Report: Meme Wars (E995)   \n",
       "\n",
       "                    author                                               text  \\\n",
       "0         David Streitfeld  PALO ALTO, Calif.  —   After years of scorning...   \n",
       "1                      NaN  Russian warships ready to strike terrorists ne...   \n",
       "2            Common Dreams  Videos #NoDAPL: Native American Leaders Vow to...   \n",
       "3            Daniel Victor  If at first you don’t succeed, try a different...   \n",
       "4  Truth Broadcast Network  42 mins ago 1 Views 0 Comments 0 Likes 'For th...   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0  [palo, alto, calif, year, scorning, political,...   \n",
       "1  [russian, warship, ready, strike, terrorist, n...   \n",
       "2  [video, #nodapl, native, american, leader, vow...   \n",
       "3  [first, succeed, try, different, sport, tim, t...   \n",
       "4  [min, ago, view, comment, like, first, time, h...   \n",
       "\n",
       "                                     processed_title  \\\n",
       "0  [specter, trump, loosens, tongue, purse, strin...   \n",
       "1  [russian, warship, ready, strike, terrorist, n...   \n",
       "2  [#nodapl, native, american, leader, vow, stay,...   \n",
       "3  [tim, tebow, attempt, another, comeback, time,...   \n",
       "4                     [keiser, report, meme, war, e]   \n",
       "\n",
       "                                          words_text  \\\n",
       "0  palo, alto, calif, year, scorning, political, ...   \n",
       "1  russian, warship, ready, strike, terrorist, ne...   \n",
       "2  video, #nodapl, native, american, leader, vow,...   \n",
       "3  first, succeed, try, different, sport, tim, te...   \n",
       "4  min, ago, view, comment, like, first, time, hi...   \n",
       "\n",
       "                                         words_title  processed_word_count  \\\n",
       "0  specter, trump, loosens, tongue, purse, string...                   765   \n",
       "1  russian, warship, ready, strike, terrorist, ne...                   152   \n",
       "2  #nodapl, native, american, leader, vow, stay, ...                   430   \n",
       "3  tim, tebow, attempt, another, comeback, time, ...                   355   \n",
       "4                       keiser, report, meme, war, e                    51   \n",
       "\n",
       "   processed_title_word_count  \n",
       "0                          11  \n",
       "1                           7  \n",
       "2                          10  \n",
       "3                          10  \n",
       "4                           5  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualização das cinco primeira notícias presentes no conjunto de treino\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF e exportação dos datasets processados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salvar os valores de target em um novo arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['label']\n",
    "pd.DataFrame(y_train, columns=['target']).to_csv(\"dataset/train_target.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "\n",
    "Como os modelos preditivos não trabalham diretamente com textos, é necessário realizar a vetorização da lista de palavras das notícias. Será utilizado o método TF-IDF para isso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(df):\n",
    "    df = df.reset_index(drop=True)\n",
    "    vectorizer = make_pipeline(\n",
    "            TfidfVectorizer(binary=True),\n",
    "            FunctionTransformer(lambda x: x.astype('float'), validate=False)\n",
    "        )\n",
    "    tfidf = vectorizer.fit_transform(df['words_text'])\n",
    "    tfidf = vectorizer.fit_transform(df['words_title'])\n",
    "    tfidf = pd.DataFrame(tfidf.toarray())\n",
    "    print (tfidf.shape)\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-d7b196050bde>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtfidf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../dataset/processed_train.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-5a792ed5a507>\u001b[0m in \u001b[0;36mtf_idf\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'words_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'words_title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\prisk\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    960\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 962\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    963\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\prisk\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1185\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1187\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size."
     ]
    }
   ],
   "source": [
    "tfidf_train = tf_idf(train)\n",
    "X_train = pd.concat([train, tfidf_train],axis=1)\n",
    "X_train.to_csv(\"../dataset/processed_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = tf_idf(test)\n",
    "X_test = pd.concat([train, tfidf_test],axis=1)\n",
    "X_test.to_csv(\"../dataset/processed_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
